# 项目运行步骤指南

## 📋 项目概述

这是一个**基于知识图谱和知识库的大模型对话系统**，主要包含三个部分：
1. **知识图谱构建**：从原始文本构建和迭代扩展知识图谱
2. **后端服务**：提供对话API和知识图谱查询API
3. **前端应用**：提供对话界面和知识图谱可视化

---

## 🔧 第一步：环境准备

### 1.1 安装Python依赖

```bash
# 在项目根目录执行
pip install -r requirements.txt
```

主要依赖包括：
- torch（PyTorch）
- transformers
- paddlenlp
- flask
- flask-cors
- 其他依赖（见 requirements.txt）

### 1.2 安装前端依赖（可选，如需运行前端）

```bash
# 进入前端目录
cd chat-kg

# 安装Node.js依赖
npm install

# 返回项目根目录
cd ..
```

---

## 📊 第二步：准备数据

### 2.1 准备原始文本数据

确保 `data/raw_data.txt` 文件存在，包含待处理的原始文本数据。

如果文件不存在，需要创建该文件并添加文本内容。

### 2.2 检查数据目录结构

确保以下目录结构存在：
```
data/
├── raw_data.txt              # 原始文本数据（必需）
├── project_v1/               # 项目数据目录（会自动创建）
└── schema/                    # 关系抽取模式定义（已存在）
```

---

## 🏗️ 第三步：构建知识图谱

### 3.1 首次运行（从零开始构建）

```bash
# 在项目根目录执行
python main.py --project project_v1 --gpu 0
```

**参数说明：**
- `--project project_v1`：项目名称，决定数据存储路径
- `--gpu 0`：指定使用的GPU ID（根据实际情况修改）

**执行流程：**
1. 从 `data/raw_data.txt` 读取原始文本
2. 使用UIE模型进行关系抽取，生成 `base.json`
3. 自动过滤无效三元组，生成 `base_filtered.json`
4. 人工筛选（需要交互），生成 `base_refined.json`
5. 开始迭代扩展知识图谱
6. 每次迭代结果保存在 `data/project_v1/iteration_v{版本号}/` 目录

### 3.2 从检查点恢复运行

如果之前运行中断，可以从检查点恢复：

```bash
# 查看可用的检查点文件
ls data/project_v1/history/

# 从检查点恢复
python main.py --project project_v1 --resume data/project_v1/history/20230327-001537_iter_v1.json --gpu 0
```

**参数说明：**
- `--resume <检查点路径>`：从指定检查点恢复运行

### 3.3 迭代过程说明

- 默认最多迭代10次（可在 `main.py` 中修改 `max_iteration`）
- 每次迭代会自动保存状态到 `data/project_v1/history/` 目录
- 当扩展比例（extend_ratio）小于1%时，自动停止迭代
- 最终知识图谱保存在 `data/project_v1/iteration_v{最新版本}/knowledge_graph.json`

---

## 🚀 第四步：启动后端服务

### 4.1 配置模型路径（重要）

在启动服务前，需要确保以下模型路径正确：

1. **ChatGLM模型路径**：编辑 `server/app/utils/chat_glm.py`
   ```python
   # 修改为你的ChatGLM模型路径
   tokenizer = AutoTokenizer.from_pretrained("/path/to/ChatGLM-6B/weights", ...)
   model = AutoModel.from_pretrained("/path/to/ChatGLM-6B/weights", ...)
   ```

2. **NER模型路径**：编辑 `server/app/utils/ner.py`
   ```python
   # 确保NER模型路径正确（默认：weights/model_41_100）
   self.model = Taskflow("ner", task_path="weights/model_41_100")
   ```

3. **知识图谱数据路径**：确保 `server/data/data.json` 存在
   - 如果不存在，需要从 `data/project_v1/iteration_v*/knowledge_graph.json` 转换格式
   - 或直接复制/转换知识图谱数据

### 4.2 启动Flask服务

```bash
# 在项目根目录执行
cd server
python main.py
```

**服务信息：**
- 服务地址：`http://0.0.0.0:8000`
- API端点：
  - `GET /` - 健康检查
  - `GET /chat/` - 聊天测试
  - `POST /chat/` - 聊天对话（流式响应）
  - `GET /graph/` - 获取知识图谱数据

**注意事项：**
- 需要GPU支持（ChatGLM模型需要较大显存，建议12GB+）
- 首次启动会加载ChatGLM模型，需要一些时间
- 确保 `CUDA_VISIBLE_DEVICES` 环境变量设置正确

---

## 🎨 第五步：启动前端应用（可选）

### 5.1 启动前端开发服务器

```bash
# 进入前端目录
cd chat-kg

# 启动开发服务器
npm run server
# 或
npm run dev
```

**前端信息：**
- 开发服务器地址：`http://localhost:5173`（默认）
- 功能：
  - 对话界面（ChatView）
  - 知识图谱可视化（GraphView）

### 5.2 配置API地址

如果后端服务地址不是默认的，需要在前端代码中修改API地址。

---

## 📝 完整运行流程示例

### 场景1：完整流程（首次运行）

```bash
# 1. 安装依赖
pip install -r requirements.txt

# 2. 确保数据文件存在
# 检查 data/raw_data.txt 是否存在

# 3. 构建知识图谱
python main.py --project project_v1 --gpu 0

# 4. 等待知识图谱构建完成（可能需要较长时间）

# 5. 准备后端数据（如果需要）
# 将知识图谱数据转换为 server/data/data.json 格式

# 6. 启动后端服务（新终端窗口）
cd server
python main.py

# 7. 启动前端（新终端窗口）
cd chat-kg
npm install
npm run server
```

### 场景2：仅运行后端服务（知识图谱已构建）

```bash
# 1. 确保知识图谱数据已准备好
# 检查 data/project_v1/iteration_v*/knowledge_graph.json

# 2. 确保 server/data/data.json 存在

# 3. 启动后端服务
cd server
python main.py
```

### 场景3：仅构建知识图谱（不启动服务）

```bash
# 直接运行知识图谱构建
python main.py --project project_v1 --gpu 0
```

---

## ⚙️ 配置说明

### GPU配置

在 `main.py` 中可以通过 `--gpu` 参数指定GPU：
```bash
python main.py --project project_v1 --gpu 1  # 使用GPU 1
```

或在代码中直接修改：
```python
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # 在main.py中修改
```

### 项目配置

- 项目名称：通过 `--project` 参数指定，默认为 `project_v1`
- 数据存储路径：`data/{project_name}/`
- 迭代版本：自动递增，从 `iteration_v0` 开始

### 迭代参数

在 `main.py` 中可以修改：
- `max_iteration`：最大迭代次数（默认10）
- `extend_ratio` 阈值：扩展比例阈值（默认0.01）

---

## 🔍 检查运行状态

### 查看知识图谱构建进度

```bash
# 查看最新迭代版本的数据
ls -la data/project_v1/iteration_v*/

# 查看运行日志
cat data/project_v1/iteration_v*/running_log.txt

# 查看知识图谱文件
cat data/project_v1/iteration_v*/knowledge_graph.json
```

### 查看后端服务状态

- 访问 `http://localhost:8000/` 检查服务是否运行
- 查看终端输出的日志信息
- 检查是否有错误信息

### 查看前端状态

- 访问 `http://localhost:5173` 检查前端是否正常加载
- 查看浏览器控制台是否有错误

---

## ❗ 常见问题

### 1. GPU内存不足

**解决方案：**
- 减少batch size
- 使用更小的模型
- 使用CPU模式（不推荐，速度很慢）

### 2. 模型路径错误

**解决方案：**
- 检查ChatGLM模型路径是否正确
- 检查NER模型路径是否正确
- 确保模型文件完整

### 3. 数据文件不存在

**解决方案：**
- 确保 `data/raw_data.txt` 存在
- 检查文件路径是否正确
- 检查文件编码是否为UTF-8

### 4. 依赖安装失败

**解决方案：**
- 使用虚拟环境：`python -m venv venv`，然后激活
- 检查Python版本（建议3.8+）
- 单独安装失败的包

### 5. 端口被占用

**解决方案：**
- 修改后端端口：编辑 `server/main.py` 中的端口号
- 修改前端端口：编辑 `chat-kg/vite.config.js`

---

## 📚 相关文档

- `CODE_READING_GUIDE.md` - 代码阅读指南
- `data/0_README.md` - 数据目录说明
- `server/0_README.md` - 服务端说明

---

## 🎯 快速开始（最小化运行）

如果只想快速测试项目，可以：

1. **仅构建知识图谱**：
   ```bash
   python main.py --project project_v1 --gpu 0
   ```

2. **查看结果**：
   ```bash
   # 查看生成的知识图谱
   cat data/project_v1/iteration_v0/knowledge_graph.json
   ```

---

**最后更新**：2024年
**项目类型**：知识图谱构建 + RAG对话系统

