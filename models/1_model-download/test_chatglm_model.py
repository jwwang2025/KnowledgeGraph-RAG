"""
å¿«é€Ÿæµ‹è¯• ChatGLM-6B æ¨¡å‹æ˜¯å¦å¯ç”¨
æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§ï¼Œå°è¯•åŠ è½½æ¨¡å‹å¹¶è¿›è¡Œç®€å•å¯¹è¯æµ‹è¯•
"""
import os
import sys
from pathlib import Path

# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent.parent
MODEL_DIR = PROJECT_ROOT / "models" / "chatglm-6b"

def check_model_files():
    """æ£€æŸ¥å…³é”®æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    print("=" * 60)
    print("æ­¥éª¤ 1: æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§")
    print("=" * 60)
    
    # å¿…éœ€çš„å…³é”®æ–‡ä»¶
    critical_files = [
        "config.json",
        "tokenizer_config.json",
        "ice_text.model",
        "pytorch_model.bin.index.json",
        "modeling_chatglm.py",
        "configuration_chatglm.py",
        "tokenization_chatglm.py",
    ]
    
    missing_files = []
    for filename in critical_files:
        file_path = MODEL_DIR / filename
        if file_path.exists():
            size_mb = file_path.stat().st_size / (1024 * 1024)
            print(f"âœ“ {filename} ({size_mb:.2f} MB)")
        else:
            print(f"âœ— {filename} (ç¼ºå¤±)")
            missing_files.append(filename)
    
    # æ£€æŸ¥æƒé‡åˆ†ç‰‡æ–‡ä»¶
    print("\næ£€æŸ¥æ¨¡å‹æƒé‡åˆ†ç‰‡æ–‡ä»¶:")
    missing_weight_files = []
    for i in range(1, 9):
        weight_file = MODEL_DIR / f"pytorch_model-{i:05d}-of-00008.bin"
        if weight_file.exists():
            size_mb = weight_file.stat().st_size / (1024 * 1024)
            print(f"âœ“ pytorch_model-{i:05d}-of-00008.bin ({size_mb:.1f} MB)")
        else:
            print(f"âœ— pytorch_model-{i:05d}-of-00008.bin (ç¼ºå¤±)")
            missing_weight_files.append(f"pytorch_model-{i:05d}-of-00008.bin")
    
    if missing_files or missing_weight_files:
        print(f"\nâš  è­¦å‘Š: å‘ç° {len(missing_files) + len(missing_weight_files)} ä¸ªç¼ºå¤±æ–‡ä»¶")
        return False
    else:
        print("\nâœ“ æ‰€æœ‰å…³é”®æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
        return True

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 2: æµ‹è¯•æ¨¡å‹åŠ è½½")
    print("=" * 60)
    
    if not MODEL_DIR.exists():
        print(f"âœ— æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {MODEL_DIR}")
        return None, None
    
    try:
        from transformers import AutoTokenizer, AutoModel
        import torch
        
        print(f"æ¨¡å‹è·¯å¾„: {MODEL_DIR}")
        print("æ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(
            str(MODEL_DIR), 
            trust_remote_code=True
        )
        print("âœ“ åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        
        print("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…ï¼‰...")
        model = AutoModel.from_pretrained(
            str(MODEL_DIR), 
            trust_remote_code=True
        )
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
        if torch.cuda.is_available():
            print(f"æ£€æµ‹åˆ° GPU: {torch.cuda.get_device_name(0)}")
            print("æ­£åœ¨å°†æ¨¡å‹ç§»è‡³ GPU...")
            model = model.half().cuda()
            print("âœ“ æ¨¡å‹å·²ç§»è‡³ GPU")
        else:
            print("âš  æœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU æ¨¡å¼ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
            model = model.float()
        
        model.eval()
        print("âœ“ æ¨¡å‹å·²è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"âœ— æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def test_chat(model, tokenizer):
    """æµ‹è¯•æ¨¡å‹å¯¹è¯åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 3: æµ‹è¯•æ¨¡å‹å¯¹è¯åŠŸèƒ½")
    print("=" * 60)
    
    if model is None or tokenizer is None:
        print("âœ— æ¨¡å‹æˆ–åˆ†è¯å™¨æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œå¯¹è¯æµ‹è¯•")
        return False
    
    try:
        # æµ‹è¯•é—®é¢˜
        test_questions = [
            "ä½ å¥½",
            "è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
        ]
        
        history = []
        
        for i, question in enumerate(test_questions, 1):
            print(f"\næµ‹è¯•é—®é¢˜ {i}: {question}")
            print("-" * 60)
            
            try:
                response, history = model.chat(tokenizer, question, history)
                print(f"æ¨¡å‹å›ç­”: {response}")
                print("âœ“ å¯¹è¯æµ‹è¯•æˆåŠŸ")
            except Exception as e:
                print(f"âœ— å¯¹è¯æµ‹è¯•å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return False
        
        print("\n" + "=" * 60)
        print("âœ“ æ‰€æœ‰å¯¹è¯æµ‹è¯•é€šè¿‡ï¼")
        print("=" * 60)
        return True
        
    except Exception as e:
        print(f"âœ— å¯¹è¯æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("\n" + "=" * 60)
    print("ChatGLM-6B æ¨¡å‹å¯ç”¨æ€§æµ‹è¯•")
    print("=" * 60)
    print(f"æ¨¡å‹ç›®å½•: {MODEL_DIR}")
    print()
    
    # æ­¥éª¤1: æ£€æŸ¥æ–‡ä»¶
    if not check_model_files():
        print("\nâš  æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´ï¼Œè¯·å…ˆå®Œæˆä¸‹è½½")
        return
    
    # æ­¥éª¤2: åŠ è½½æ¨¡å‹
    model, tokenizer = test_model_loading()
    if model is None or tokenizer is None:
        print("\nâœ— æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        return
    
    # æ­¥éª¤3: æµ‹è¯•å¯¹è¯
    success = test_chat(model, tokenizer)
    
    # æœ€ç»ˆç»“æœ
    print("\n" + "=" * 60)
    if success:
        print("ğŸ‰ æµ‹è¯•å®Œæˆï¼šæ¨¡å‹å¯ç”¨ï¼")
        print("=" * 60)
        print("\næ¨¡å‹å·²æˆåŠŸåŠ è½½å¹¶å¯ä»¥æ­£å¸¸å¯¹è¯ã€‚")
        print("ä½ ç°åœ¨å¯ä»¥åœ¨é¡¹ç›®ä¸­ä½¿ç”¨è¿™ä¸ªæ¨¡å‹äº†ã€‚")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥ï¼šæ¨¡å‹å­˜åœ¨é—®é¢˜")
        print("=" * 60)
        print("\nè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤é—®é¢˜ã€‚")
    print()

if __name__ == "__main__":
    main()

